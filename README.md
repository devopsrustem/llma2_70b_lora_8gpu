# MLPerf Llama 2 70B LoRA –Ω–∞ DGX H100: TLDR
```
cd /scratch/workdir/training_results_v5.0/NVIDIA/benchmarks/llama2_70b_lora/implementations/tyche_ngpu8_ngc25.04_nemo

enroot import docker://mlperf-nvidia:llama2_70b_lora-pyt
# make files config_H100_final.sh & run_h100_universal.sub (see git example)
source config_H100_final.sh

sbatch -N $DGXNNODES -t $WALLTIME run_h100_universal.sub

tail -f slurm-69.out

while true; do   date;   sq && grep "eval_accuracy" slurm-70.out;   echo "---------------------";   sleep 60; done | tee -a monitor.log

```

train_loss: 1.3589 - –ø–æ—Ç–µ—Ä–∏ –ø—Ä–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ (—á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –ª—É—á—à–µ)
samples_count: 430 - —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ 430 –æ–±—Ä–∞–∑—Ü–æ–≤
lr: 0.00022 - —Ç–µ–∫—É—â–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (learning rate)
time_ms: 1756976006303 - –≤—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞

- `eval_accuracy: 0.9457` - —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ
- **–¶–ï–õ–¨ MLPerf**: –∑–Ω–∞—á–µ–Ω–∏–µ ‚â§ 0.925
- –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è —É—Å–ø–µ—à–Ω–æ –∫–æ–≥–¥–∞ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è —Ü–µ–ª—å

# MLPerf Llama 2 70B LoRA –Ω–∞ DGX H100: –ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã

![MLPerf Architecture](./docs/mlperf-architecture.png)
*–°—Ö–µ–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è Llama 2 70B —Å LoRA –Ω–∞ DGX H100*

## –û–±–∑–æ—Ä
–≠—Ç–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∫–∞–∫ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—å –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å MLPerf v5.0 Llama 2 70B LoRA benchmark –Ω–∞ DGX H100 —Å–∏—Å—Ç–µ–º–µ —Å SLURM –∫–ª–∞—Å—Ç–µ—Ä–æ–º.

## –ò—Å—Ö–æ–¥–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
–ë–∞–∑–∏—Ä—É–µ—Ç—Å—è –Ω–∞: https://developer.nvidia.com/blog/reproducing-nvidia-mlperf-v5-0-training-scores-for-llm-benchmarks/

## –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

### –û–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ
- DGX H100 —Å 8x NVIDIA H100 80GB GPU
- –ú–∏–Ω–∏–º—É–º 300GB –¥–∏—Å–∫–æ–≤–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
- InfiniBand —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

### –ü—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ
- SLURM workload manager
- Enroot container runtime
- Pyxis SLURM plugin –¥–ª—è –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
- Docker –¥–ª—è —Å–±–æ—Ä–∫–∏ –æ–±—Ä–∞–∑–æ–≤

## –ü–æ—à–∞–≥–æ–≤–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞

### 1. –ö–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
```bash
git clone https://github.com/mlcommons/training_results_v5.0.git
cd training_results_v5.0/NVIDIA/benchmarks/llama2_70b_lora/implementations/tyche_ngpu8_ngc25.04_nemo
```

### 2. –°–±–æ—Ä–∫–∞ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
```bash
docker build -t mlperf-nvidia:llama2_70b_lora-pyt .
```

### 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
```bash
# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö
mkdir -p /scratch/workdir/mlperf-datasets

# –ó–∞–ø—É—Å–∫ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
docker run -it --rm --gpus all --network=host --ipc=host \
  --volume /scratch/workdir/mlperf-datasets:/data \
  mlperf-nvidia:llama2_70b_lora-pyt

# –í–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞:
python scripts/download_dataset.py --data_dir /data/gov_report
python scripts/download_model.py --model_dir /data/model
```

### 4. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Enroot
```bash
# –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
sudo mkdir -p /run/enroot/user-$(id -u)
sudo chown $(id -u):$(id -g) /run/enroot/user-$(id -u)
mkdir -p /tmp/enroot-data/user-$(id -u)
sudo mkdir -p /var/lib/enroot-cache/group-$(id -g)
sudo chown $(id -u):$(id -g) /var/lib/enroot-cache/group-$(id -g)

# –ò–º–ø–æ—Ä—Ç –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ –≤ Enroot
enroot import dockerd://mlperf-nvidia:llama2_70b_lora-pyt
```

### 5. –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ SLURM/Pyxis

#### –ü—Ä–æ–±–ª–µ–º–∞
–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Å–∫—Ä–∏–ø—Ç—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Å–∏–Ω—Ç–∞–∫—Å–∏—Å Pyxis –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å —Å–æ –≤—Å–µ–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ SLURM.

#### –†–µ—à–µ–Ω–∏–µ
–°–æ–∑–¥–∞—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é `run.sub`:

```bash
cp run.sub run_h100_universal.sub

# –ó–∞–º–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö srun –∫–æ–º–∞–Ω–¥ –Ω–∞ —Ä–∞–±–æ—á–∏–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å
sed -i 's/srun -N1 -n1 --container-name="${_cont_name}" "${PYXIS_DEFAULTS\[@\]}"/srun --ntasks-per-node=1 --gres=gpu:8 --cpu-bind=none --mpi=pmix --container-image=".\/mlperf-nvidia+llama2_70b_lora-pyt.sqsh"/g' run_h100_universal.sub

# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–º–∞–Ω–¥—ã —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
sed -i '218,223c\
        srun --ntasks-per-node=8 --gres=gpu:8 --cpu-bind=none --mpi=pmix \\\
         --container-image="./mlperf-nvidia+llama2_70b_lora-pyt.sqsh" \\\
         --container-mounts="${_cont_mounts}" \\\
         --container-workdir="${WORK_DIR}" \\\
         --time="${WALLTIME_RUNANDTIME}" \\\
             slurm2pytorch ./run_and_time.sh' run_h100_universal.sub
```

### 6. –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ H100

```bash
cat > config_H100_optimal.sh << 'EOF'
#!/bin/bash
source $(dirname ${BASH_SOURCE[0]})/config_common.sh

# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
export MAX_STEPS=1000
export LR=0.0005
export MINIBS=1

# –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è –¥–ª—è H100 DGX (8 GPU)
export TP=8      # Tensor Parallel - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ 8 GPU
export PP=1      # Pipeline Parallel - –±–µ–∑ pipeline
export CP=1      # Context Parallel - –±–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
export FP8_ACT=1 # FP8 –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
export VAL_CHECK_INTERVAL=50  # –ß–∞—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
export LIMIT_VAL_BATCHES=1.0

# –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
export VBOOST_VALUE=0
export DGXNNODES=1
export DGXNGPU=8
export WALLTIME_RUNANDTIME=80
export WALLTIME=$((5 + ${NEXP:-1} * ($WALLTIME_RUNANDTIME + 5)))
EOF

chmod +x config_H100_optimal.sh
```

### 7. –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞

```bash
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π
export DATADIR="/scratch/workdir/mlperf-datasets/gov_report"
export MODEL="/scratch/workdir/mlperf-datasets/model"
export LOGDIR="/scratch/workdir/mlperf_logs"
export CONT="./mlperf-nvidia+llama2_70b_lora-pyt.sqsh"

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
source config_H100_optimal.sh

# –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞
sbatch -N $DGXNNODES -t $WALLTIME run_h100_universal.sub
```

## –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ MLPerf

#### 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
```
:::MLLOG {"event_type": "INTERVAL_START", "key": "init_start"}
```
**–ó–Ω–∞—á–µ–Ω–∏–µ**: –ù–∞—á–∞–ª–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω—ã—Ö

#### 2. –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
```
:::MLLOG {"key": "opt_base_learning_rate", "value": 0.0005}
:::MLLOG {"key": "lora_rank", "value": 16}
:::MLLOG {"key": "lora_alpha", "value": 32}
:::MLLOG {"key": "global_batch_size", "value": 8}
```
**–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞**:
- `opt_base_learning_rate: 0.0005` - –±–∞–∑–æ–≤–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
- `lora_rank: 16` - —Ä–∞–Ω–≥ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ (—Ä–∞–∑–º–µ—Ä –∞–¥–∞–ø—Ç–∞—Ü–∏–∏)
- `lora_alpha: 32` - scaling factor –¥–ª—è LoRA
- `global_batch_size: 8` - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞

#### 3. –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è
```
:::MLLOG {"key": "run_start"}  # –ù–∞—á–∞–ª–æ –∏–∑–º–µ—Ä–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏
:::MLLOG {"key": "train_loss", "value": 1.3589, "samples_count": 430, "lr": 0.00022}
```
**–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞**:
- `train_loss: 1.3589` - –ø–æ—Ç–µ—Ä–∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ (–¥–æ–ª–∂–Ω—ã —Å–Ω–∏–∂–∞—Ç—å—Å—è)
- `samples_count: 430` - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
- `lr: 0.00022` - —Ç–µ–∫—É—â–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (—Å learning rate decay)

#### 4. –í–∞–ª–∏–¥–∞—Ü–∏—è (–∫–ª—é—á–µ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞)
```
:::MLLOG {"key": "eval_accuracy", "value": 0.9457, "samples_count": 576}
```
**–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞**:
- `eval_accuracy: 0.9457` - —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ
- **–¶–ï–õ–¨ MLPerf**: –∑–Ω–∞—á–µ–Ω–∏–µ ‚â§ 0.925
- –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è —É—Å–ø–µ—à–Ω–æ –∫–æ–≥–¥–∞ –¥–æ—Å—Ç–∏–≥–∞–µ—Ç—Å—è —Ü–µ–ª—å

#### 5. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
```
:::MLLOG {"key": "run_stop", "status": "success"}
```
**–í–æ–∑–º–æ–∂–Ω—ã–µ —Å—Ç–∞—Ç—É—Å—ã**:
- `"success"` - —Ü–µ–ª—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞, —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞–ª–∏–¥–µ–Ω
- `"aborted"` - –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –±–µ–∑ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–∏

### –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

#### NCCL —Ç–µ—Å—Ç (–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è –º–µ–∂–¥—É GPU)
```
# Avg bus bandwidth: 334.536 GB/s
```
**–•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**: >300 GB/s –¥–ª—è H100 DGX

#### –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –∫–∞–∫ —Ä–∞–∑–Ω–æ—Å—Ç—å –º–µ–∂–¥—É `run_stop` –∏ `run_start`:
```bash
grep "run_start\|run_stop" slurm-*.out
# –ü—Ä–∏–º–µ—Ä: 91459 –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥ = 1.524 –º–∏–Ω—É—Ç—ã
```

**–≠—Ç–∞–ª–æ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è**:
- NVIDIA GB200: ~1.5-3 –º–∏–Ω—É—Ç—ã
- DGX H100: –æ–∂–∏–¥–∞–µ–º–æ 5-15 –º–∏–Ω—É—Ç

## –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è

### 1. SPANK assertion failed
**–°–∏–º–ø—Ç–æ–º**: `srun: error: spank.c:1154: *do*option_cb(): Assertion (arg) failed`
**–†–µ—à–µ–Ω–∏–µ**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å srun —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º GPU —Ä–µ—Å—É—Ä—Å–æ–≤

### 2. CUDA out of memory
**–°–∏–º–ø—Ç–æ–º**: `torch.OutOfMemoryError: CUDA out of memory`
**–†–µ—à–µ–Ω–∏–µ**: –£–≤–µ–ª–∏—á–∏—Ç—å Tensor Parallelism (TP=8) –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏

### 3. val_check_interval –±–æ–ª—å—à–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –±–∞—Ç—á–µ–π
**–°–∏–º–ø—Ç–æ–º**: `val_check_interval (1152) must be less than or equal to the number of the training batches`
**–†–µ—à–µ–Ω–∏–µ**: –£–º–µ–Ω—å—à–∏—Ç—å `VAL_CHECK_INTERVAL` –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

### 4. –ú–æ–¥–µ–ª—å –Ω–µ —Å—Ö–æ–¥–∏—Ç—Å—è (eval_accuracy –∑–∞—Å—Ç—Ä—è–ª)
**–°–∏–º–ø—Ç–æ–º—ã**: 
- eval_accuracy –∫–æ–ª–µ–±–ª–µ—Ç—Å—è –æ–∫–æ–ª–æ –æ–¥–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
- –ù–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∫ —Ü–µ–ª–∏ ‚â§ 0.925
**–†–µ—à–µ–Ω–∏—è**:
- –£–≤–µ–ª–∏—á–∏—Ç—å `MAX_STEPS`
- –ù–∞—Å—Ç—Ä–æ–∏—Ç—å learning rate
- –£–≤–µ–ª–∏—á–∏—Ç—å —á–∞—Å—Ç–æ—Ç—É –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (`VAL_CHECK_INTERVAL`)

## –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–∏—Å—Ç–µ–º

### –î–ª—è –º–µ–Ω—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ GPU
```bash
export TP=4    # –î–ª—è 4 GPU
export TP=2    # –î–ª—è 2 GPU
export TP=1    # –î–ª—è 1 GPU (–ø–æ—Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏)
```

### –î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
```bash
export MAX_STEPS=100      # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
export VAL_CHECK_INTERVAL=25  # –ß–∞—Å—Ç–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
```

## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
```bash
squeue                    # –°—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á SLURM
nvidia-smi               # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU
tail -f slurm-*.out      # –õ–æ–≥–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
```

### –ö–ª—é—á–µ–≤—ã–µ –ª–æ–≥–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
```bash
# –ü—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è
tail -f slurm-*.out | grep "train_loss"

# –í–∞–ª–∏–¥–∞—Ü–∏—è (—Å–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ)
tail -f slurm-*.out | grep "eval_accuracy"

# –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
grep "run_stop" slurm-*.out
```

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–£—Å–ø–µ—à–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ MLPerf Llama 2 70B LoRA benchmark –Ω–∞ DGX H100 –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç:
- –ö–æ—Ä—Ä–µ–∫—Ç–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
- –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é —Ä–∞–±–æ—Ç—É GPU –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã
- –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å–∏—Å—Ç–µ–º—ã —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º ML workloads

–¢–∏–ø–∏—á–Ω–æ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–∞ H100 DGX: 10-20 –º–∏–Ω—É—Ç –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ —Ü–µ–ª–µ–≤–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ eval_accuracy ‚â§ 0.925.


# –ù–∏–∑–∫–∏–µ –û –≤—ã—Å–æ–∫–æ–º
üîπ –ß—Ç–æ —Ç–∞–∫–æ–µ —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å

–°—Ö–æ–¥–∏–º–æ—Å—Ç—å ‚Äî —ç—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å, –∫–æ–≥–¥–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –æ—à–∏–±–∫–∞ (loss) –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è, –∞ –∫–∞—á–µ—Å—Ç–≤–æ (accuracy, eval_accuracy –∏ —Ç.–¥.) —Ä–∞—Å—Ç—ë—Ç –∏ –≤—ã—Ö–æ–¥–∏—Ç –Ω–∞ ¬´–ø–ª–∞—Ç–æ¬ª.

–ü—Ä–∏–º–µ—Ä:

–í –Ω–∞—á–∞–ª–µ —Å–µ—Ç—å ¬´—É–≥–∞–¥—ã–≤–∞–µ—Ç —Å–ª—É—á–∞–π–Ω–æ¬ª ‚Üí —Ç–æ—á–Ω–æ—Å—Ç—å 10‚Äì20%.

–ü–æ—Ç–æ–º –Ω–∞—á–∏–Ω–∞–µ—Ç –æ–±—É—á–∞—Ç—å—Å—è, –æ—à–∏–±–∫–∞ —Å–Ω–∏–∂–∞–µ—Ç—Å—è, —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–∞—Å—Ç—ë—Ç.

–ù–∞ –∫–∞–∫–æ–º-—Ç–æ —ç—Ç–∞–ø–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ—á—Ç–∏ –Ω–µ —É–ª—É—á—à–∞–µ—Ç—Å—è ‚Üí –º–æ–¥–µ–ª—å ¬´—Å–æ—à–ª–∞—Å—å¬ª.

üëâ –ï—Å–ª–∏ —Å–µ—Ç—å –Ω–µ —Å—Ö–æ–¥–∏—Ç—Å—è, —ç—Ç–æ –∑–Ω–∞—á–∏—Ç:

–æ—à–∏–±–∫–∞ –∫–æ–ª–µ–±–ª–µ—Ç—Å—è –∏–ª–∏ –¥–∞–∂–µ —Ä–∞—Å—Ç—ë—Ç;

—Ç–æ—á–Ω–æ—Å—Ç—å –Ω–µ —Ä–∞—Å—Ç—ë—Ç, –æ—Å—Ç–∞—ë—Ç—Å—è –Ω–∏–∑–∫–æ–π.

üîπ –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏

–ú–æ–∂–Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å, —á—Ç–æ –æ–±—É—á–µ–Ω–∏–µ ‚Äî —ç—Ç–æ –∫–∞–∫ —Å–ø—É—Å–∫ —Å –≥–æ—Ä—ã –≤ –¥–æ–ª–∏–Ω—É, —Ç–æ–ª—å–∫–æ –≥–æ—Ä–∞ ‚Äî —ç—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è –æ—à–∏–±–∫–∏ (loss).

–£ –Ω–∞—Å –µ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (–≤–µ—Å–∞ –Ω–µ–π—Ä–æ–Ω–æ–≤).

–ú—ã —Å—á–∏—Ç–∞–µ–º –æ—à–∏–±–∫—É (loss) –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

–ê–ª–≥–æ—Ä–∏—Ç–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ —Ä–µ—à–∞–µ—Ç: ¬´–í –∫–∞–∫—É—é —Å—Ç–æ—Ä–æ–Ω—É —Å–¥–≤–∏–Ω—É—Ç—å –≤–µ—Å–∞, —á—Ç–æ–±—ã –æ—à–∏–±–∫–∞ —É–º–µ–Ω—å—à–∏–ª–∞—Å—å¬ª.

–ú–∞–ª–µ–Ω—å–∫–∏–π —à–∞–≥ ‚Üí –∏–¥—ë–º –º–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ —Å—Ç–∞–±–∏–ª—å–Ω–æ.

–ë–æ–ª—å—à–æ–π —à–∞–≥ ‚Üí –º–æ–∂–µ–º –±—ã—Å—Ç—Ä–æ —Å–∫–∞—Ç–∏—Ç—å—Å—è –≤–Ω–∏–∑, –Ω–æ –∏–Ω–æ–≥–¥–∞ –ø—Ä–æ—Å–∫–æ—á–∏–º –º–∏–º–æ –∏ —É–ª–µ—Ç–∏–º –≤ ¬´–∫—É—Å—Ç–∏–∫–∏¬ª (–¥–∏–≤–µ—Ä–≥–∏—Ä—É–µ–º).

üîπ –ì–ª–∞–≤–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏

Learning Rate (LR) ‚Äî —Ä–∞–∑–º–µ—Ä —à–∞–≥–∞ –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–º —Å–ø—É—Å–∫–µ.

–°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π ‚Üí —É—á–∏–º—Å—è –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ.

–°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ‚Üí –º–æ–¥–µ–ª—å ¬´—Å–∫–∞—á–µ—Ç¬ª –∏ –Ω–µ —É—á–∏—Ç—Å—è.

Batch Size (MINIBS) ‚Äî —Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –º—ã –±–µ—Ä—ë–º –¥–ª—è –æ–¥–Ω–æ–≥–æ —à–∞–≥–∞.

–ú–∞–ª—ã–π –±–∞—Ç—á = —à—É–º–Ω—ã–µ —à–∞–≥–∏, –Ω–æ –ª—É—á—à–µ –æ–±–æ–±—â–µ–Ω–∏–µ.

–ë–æ–ª—å—à–æ–π –±–∞—Ç—á = —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ, –Ω–æ –º–æ–∂–µ—Ç –∑–∞—Å—Ç—Ä—è—Ç—å.

Weight Decay ‚Äî ¬´—à—Ç—Ä–∞—Ñ¬ª –∑–∞ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –≤–µ—Å–∞ (—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è).

–ù–µ –¥–∞—ë—Ç —Å–µ—Ç–∏ –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å—Å—è.

Gradient Clipping ‚Äî –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤.

–ó–∞—â–∏—Ç–∞ –æ—Ç ¬´–≤–∑—Ä—ã–≤–æ–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞¬ª, –∫–æ–≥–¥–∞ –≤—Å—ë –ª–æ–º–∞–µ—Ç—Å—è.

Scheduler (LR decay) ‚Äî –º–µ–Ω—è–µ–º LR –≤–æ –≤—Ä–µ–º–µ–Ω–∏.

–û–±—ã—á–Ω–æ: —Å–Ω–∞—á–∞–ª–∞ warmup (–º–µ–¥–ª–µ–Ω–Ω–æ —Ä–∞–∑–≥–æ–Ω—è–µ–º—Å—è),
–ø–æ—Ç–æ–º –ø–ª–∞–≤–Ω–æ–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ (—á—Ç–æ–±—ã –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –¥–æ–π—Ç–∏ –¥–æ –º–∏–Ω–∏–º—É–º–∞).

üîπ –ü—Ä–∏–º–µ—Ä –∏–∑ –∂–∏–∑–Ω–∏

–ü—Ä–µ–¥—Å—Ç–∞–≤—å: —Ç—ã —É—á–∏—à—å—Å—è –∏–≥—Ä–∞—Ç—å –≤ –¥–∞—Ä—Ç—Å üéØ.

Loss = –Ω–∞—Å–∫–æ–ª—å–∫–æ –¥–∞–ª–µ–∫–æ —Ç–≤–æ—è —Å—Ç—Ä–µ–ª–∞ –æ—Ç —Ü–µ–Ω—Ç—Ä–∞.

LR = –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–∏–ª—å–Ω–æ —Ç—ã –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—à—å –±—Ä–æ—Å–æ–∫ –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏.

Batch = —Å–∫–æ–ª—å–∫–æ –±—Ä–æ—Å–∫–æ–≤ –¥–µ–ª–∞–µ—à—å, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, –∫–∞–∫ —É–ª—É—á—à–∏—Ç—å.

Weight Decay = –Ω–µ –¥–∞—ë—Ç —Ç–µ–±–µ ¬´—Ä–∞–∑–º–∞—Ö–∏–≤–∞—Ç—å—Å—è —Å–ª–∏—à–∫–æ–º —Å–∏–ª—å–Ω–æ¬ª.

Gradient Clipping = –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º —Ä–µ–∑–∫–æ –∫–∏–¥–∞–µ—à—å ‚Äî –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —Å–∏–ª—É.

Scheduler = —Å–Ω–∞—á–∞–ª–∞ —É—á–∏—à—å—Å—è –±—ã—Å—Ç—Ä–æ, –∞ –ø–æ—Ç–æ–º —à–ª–∏—Ñ—É–µ—à—å –º–µ–ª–æ—á–∏ –º–µ–¥–ª–µ–Ω–Ω–µ–µ.

üîπ –ò—Ç–æ–≥

–°—Ö–æ–¥–∏–º–æ—Å—Ç—å ‚Äî —ç—Ç–æ –ø—É—Ç—å –º–æ–¥–µ–ª–∏ –∫ ¬´—Ä–∞–∑—É–º–Ω–æ–π¬ª —Ç–æ—á–Ω–æ—Å—Ç–∏.

–í –∏–¥–µ–∞–ª–µ –≥—Ä–∞—Ñ–∏–∫ loss –ø–∞–¥–∞–µ—Ç, –∞ accuracy —Ä–∞—Å—Ç—ë—Ç.

–ï—Å–ª–∏ —ç—Ç–æ–≥–æ –Ω–µ—Ç ‚Üí –∑–Ω–∞—á–∏—Ç, –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–ª–∏ –¥–∞–Ω–Ω—ã–µ –ø–æ–¥–æ–±—Ä–∞–Ω—ã –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ.


## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã

![MLPerf Architecture](./docs/1.png)
*–°—Ö–µ–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è Llama 2 70B —Å LoRA –Ω–∞ DGX H100*
–≠—Ç–æ –∏ –µ—Å—Ç—å –æ—Å–Ω–æ–≤—ã —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏:

üîµ Loss (–æ—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏)

–ü—Ä–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ (–∑–µ–ª—ë–Ω—ã–π) –æ—à–∏–±–∫–∞ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è –∏ –≤—ã—Ö–æ–¥–∏—Ç –Ω–∞ ¬´–ø–ª–∞—Ç–æ¬ª.

–ü—Ä–∏ —Ä–∞—Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ (–∫—Ä–∞—Å–Ω—ã–π) –æ—à–∏–±–∫–∞ –Ω–µ —É–±—ã–≤–∞–µ—Ç ‚Äî –æ–Ω–∞ –±–æ–ª—Ç–∞–µ—Ç—Å—è –∏–ª–∏ –¥–∞–∂–µ —Ä–∞—Å—Ç—ë—Ç.

üîµ Accuracy (—Ç–æ—á–Ω–æ—Å—Ç—å)

–ü—Ä–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ (–∑–µ–ª—ë–Ω—ã–π) —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–∞—Å—Ç—ë—Ç –∏ —Ç–æ–∂–µ –≤—ã—Ö–æ–¥–∏—Ç –Ω–∞ –ø–ª–∞—Ç–æ.

–ü—Ä–∏ —Ä–∞—Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏ (–∫—Ä–∞—Å–Ω—ã–π) –º–æ–¥–µ–ª—å –Ω–µ —É—á–∏—Ç—Å—è: —Ç–æ—á–Ω–æ—Å—Ç—å –∫–æ–ª–µ–±–ª–µ—Ç—Å—è –∏–ª–∏ –æ—Å—Ç–∞—ë—Ç—Å—è –Ω–∏–∑–∫–æ–π.

üëâ –°—Ö–æ–¥–∏–º–æ—Å—Ç—å = –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –ø—Ä–∏–±–ª–∏–∂–∞—é—Ç—Å—è –∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º.
üëâ –†–∞—Å—Ö–æ–¥–∏–º–æ—Å—Ç—å = –º–æ–¥–µ–ª—å –Ω–µ —É—á–∏—Ç—Å—è, –æ–±—É—á–µ–Ω–∏–µ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ (—Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π learning rate, –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö, –ø–ª–æ—Ö–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è).
